{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "- load raw post data from json file\n",
    "- convert json data into dataframe\n",
    "- merge post 'text' with emotion labels\n",
    "- split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:20.537758Z",
     "iopub.status.busy": "2025-12-01T12:24:20.537527Z",
     "iopub.status.idle": "2025-12-01T12:24:23.793850Z",
     "shell.execute_reply": "2025-12-01T12:24:23.793130Z",
     "shell.execute_reply.started": "2025-12-01T12:24:20.537736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (47890, 5)\n",
      "test_df: (16281, 4)\n",
      "         id                                               text hashtags  \\\n",
      "0  0x35663e  I bet there is an army of married couples who ...            \n",
      "1  0xc78afe                         This could only end badly.            \n",
      "2  0x90089c  My sister squeezed a lime in her milk when she...            \n",
      "3  0x2ffb63                                Thank you so muchâ¤ï¸            \n",
      "4  0x989146  Stinks because ive been in this program for a ...            \n",
      "\n",
      "   split emotion  \n",
      "0  train     joy  \n",
      "1  train    fear  \n",
      "2  train     joy  \n",
      "3  train     joy  \n",
      "4  train     joy  \n",
      "         id                                               text hashtags split\n",
      "0  0x61fc95  We got the ranch, loaded our guns and sat up t...           test\n",
      "4  0xaba820         and that got my head bobbing a little bit.           test\n",
      "5  0x66e44d                Same. Glad it's not just out store.           test\n",
      "6  0xc03cf5  Like always i will wait and see thanks for the...           test\n",
      "8  0x02f65a  There's a bit of room between \"not loving sub-...           test\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "path = \"/kaggle/input/dm-lab-2-private-competition\"\n",
    "\n",
    "# load raw post data from json\n",
    "with open(f\"{path}/final_posts.json\", \"r\", encoding = \"utf-8\") as f:\n",
    "    raw_posts = json.load(f)\n",
    "\n",
    "# json to dataframe\n",
    "posts = []\n",
    "for item in raw_posts:\n",
    "    post = item[\"root\"][\"_source\"][\"post\"]\n",
    "    posts.append({\n",
    "        \"id\": post[\"post_id\"],\n",
    "        \"text\": post[\"text\"],\n",
    "        \"hashtags\": \" \".join(post.get(\"hashtags\", []))\n",
    "    })\n",
    "posts_df = pd.DataFrame(posts)\n",
    "\n",
    "# load emotion labels and data split\n",
    "emotion_df = pd.read_csv(f\"{path}/emotion.csv\")\n",
    "split_df = pd.read_csv(f\"{path}/data_identification.csv\")\n",
    "\n",
    "# train/test dataframe\n",
    "merged = posts_df.merge(split_df, on = \"id\", how = \"inner\")\n",
    "train_df = merged[merged[\"split\"] == \"train\"].merge(emotion_df, on = \"id\", how = \"left\")\n",
    "test_df = merged[merged[\"split\"] == \"test\"]\n",
    "\n",
    "print(\"train_df:\", train_df.shape)\n",
    "print(\"test_df:\", test_df.shape)\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic checks\n",
    "\n",
    "Basic check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:23.795882Z",
     "iopub.status.busy": "2025-12-01T12:24:23.795328Z",
     "iopub.status.idle": "2025-12-01T12:24:23.818989Z",
     "shell.execute_reply": "2025-12-01T12:24:23.818359Z",
     "shell.execute_reply.started": "2025-12-01T12:24:23.795859Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df missing values:\n",
      "id          0\n",
      "text        0\n",
      "hashtags    0\n",
      "split       0\n",
      "emotion     0\n",
      "dtype: int64\n",
      "\n",
      "test_df missing values:\n",
      "id          0\n",
      "text        0\n",
      "hashtags    0\n",
      "split       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train_df missing values:\")\n",
    "print(train_df.isna().sum())\n",
    "\n",
    "print(\"\\ntest_df missing values:\")\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the class distribution of the emotion labels in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:23.820132Z",
     "iopub.status.busy": "2025-12-01T12:24:23.819805Z",
     "iopub.status.idle": "2025-12-01T12:24:23.836837Z",
     "shell.execute_reply": "2025-12-01T12:24:23.836208Z",
     "shell.execute_reply.started": "2025-12-01T12:24:23.820102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotion\n",
      "joy         23797\n",
      "anger       10694\n",
      "surprise     6281\n",
      "sadness      3926\n",
      "fear         2009\n",
      "disgust      1183\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "Analyze the common patterns, such as emails, user mentions, and multiple whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:23.837727Z",
     "iopub.status.busy": "2025-12-01T12:24:23.837503Z",
     "iopub.status.idle": "2025-12-01T12:24:24.118600Z",
     "shell.execute_reply": "2025-12-01T12:24:24.118053Z",
     "shell.execute_reply.started": "2025-12-01T12:24:23.837711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of rows with emails\n",
      "60\n",
      "\n",
      "number of rows with @usernames\n",
      "2410\n",
      "\n",
      "number of rows with multiple whitespaces\n",
      "596\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# count occurrences of pattern\n",
    "def count_pattern(df, pattern):\n",
    "    return df[\"text\"].str.contains(pattern, regex = True).sum()\n",
    "\n",
    "print(\"\\nnumber of rows with emails\")\n",
    "print(count_pattern(train_df, r\"\\S+@\\S+\"))\n",
    "\n",
    "print(\"\\nnumber of rows with @usernames\")\n",
    "print(count_pattern(train_df, r\"@\\w+\"))\n",
    "\n",
    "print(\"\\nnumber of rows with multiple whitespaces\")\n",
    "print(count_pattern(train_df, r\"\\s{2,}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **clean_text** function to remove emails, user mentions, and compress multiple whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:24.119471Z",
     "iopub.status.busy": "2025-12-01T12:24:24.119243Z",
     "iopub.status.idle": "2025-12-01T12:24:24.749176Z",
     "shell.execute_reply": "2025-12-01T12:24:24.748431Z",
     "shell.execute_reply.started": "2025-12-01T12:24:24.119453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  \\\n",
      "32473               >YOUR STATEMENT I'm not that person.   \n",
      "2494    Do you have any relatives who could support you?   \n",
      "41963                                       Wtf? Really?   \n",
      "29666  In our county, 2018 was the turning pointâ€”then...   \n",
      "15806  All good brother. Los Lonely Boys just came on...   \n",
      "42109      Haven't printed a retraction in twenty years!   \n",
      "47596  Heroin definetley isn't safer than Ritalin. Th...   \n",
      "37756  I like your colour choices and I can feel your...   \n",
      "5697   I'd enjoy a nice warm hug. Along with a deligh...   \n",
      "3564                                  A royal with creme   \n",
      "\n",
      "                                              clean_text   emotion  \n",
      "32473               >YOUR STATEMENT I'm not that person.       joy  \n",
      "2494    Do you have any relatives who could support you?       joy  \n",
      "41963                                       Wtf? Really?     anger  \n",
      "29666  In our county, 2018 was the turning pointâ€”then...     anger  \n",
      "15806  All good brother. Los Lonely Boys just came on...       joy  \n",
      "42109      Haven't printed a retraction in twenty years!  surprise  \n",
      "47596  Heroin definetley isn't safer than Ritalin. Th...       joy  \n",
      "37756  I like your colour choices and I can feel your...       joy  \n",
      "5697   I'd enjoy a nice warm hug. Along with a deligh...       joy  \n",
      "3564                                  A royal with creme       joy  \n",
      "\n",
      "train_df: (47890, 6)\n",
      "test_df: (16281, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/2677140072.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.loc[:, \"clean_text\"] = test_df[\"text\"].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "def clean_text(s):\n",
    "    #s = s.lower()\n",
    "    s = re.sub(r\"\\S+@\\S+\", \"\", s) # remove emails\n",
    "    s = re.sub(r\"@\\w+\", \"\", s) # remove @usernames\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip() # compress multiple spaces into one\n",
    "    return s\n",
    "\n",
    "# apply the function and store results in a new column 'clean_text'\n",
    "train_df.loc[:, \"clean_text\"] = train_df[\"text\"].apply(clean_text)\n",
    "test_df.loc[:, \"clean_text\"] = test_df[\"text\"].apply(clean_text)\n",
    "\n",
    "print(train_df[[\"text\", \"clean_text\", \"emotion\"]].sample(10, random_state = 42))\n",
    "print(\"\\ntrain_df:\", train_df.shape)\n",
    "print(\"test_df:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a sample of original and cleaned text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:24.750036Z",
     "iopub.status.busy": "2025-12-01T12:24:24.749804Z",
     "iopub.status.idle": "2025-12-01T12:24:25.014795Z",
     "shell.execute_reply": "2025-12-01T12:24:25.014227Z",
     "shell.execute_reply.started": "2025-12-01T12:24:24.750012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "remove email\n",
      "\n",
      "sample 1\n",
      "original: Profit are from 5-20% per day to 100-300% per month.Don't hesitate to contact us for the more results! info@fxventury.com\n",
      "cleaned : Profit are from 5-20% per day to 100-300% per month.Don't hesitate to contact us for the more results!\n",
      "\n",
      "sample 2\n",
      "original: Go follow #beautiful #Snowgang â™¥@Amynicolehill12 â™¥ #Princess #fitness #bodyposi #haircut #smile #Whitegirlwednesday\n",
      "cleaned : Go follow #beautiful #Snowgang â™¥ #Princess #fitness #bodyposi #haircut #smile #Whitegirlwednesday\n",
      "\n",
      "sample 3\n",
      "original: .@billradkeradio is not a fan of The Beat Happening. But that's not to discourage aspiring other Olympia musicians! #KUOWrecord\n",
      "cleaned : is not a fan of The Beat Happening. But that's not to discourage aspiring other Olympia musicians! #KUOWrecord\n",
      "\n",
      "sample 4\n",
      "original: .@Travelanswerman: The possibilities R endless Becca when U unplug from the 'Matrix!' Stay salty and #happy #surfing! #sunset #aloha â€¦\n",
      "cleaned : The possibilities R endless Becca when U unplug from the 'Matrix!' Stay salty and #happy #surfing! #sunset #aloha â€¦\n",
      "\n",
      "sample 5\n",
      "original: Bes! You don't just tell a true blooded hoopjunkie to switch a f*c@n' team that juz destroyed your own team. You juz don't! \n",
      "cleaned : Bes! You don't just tell a true blooded hoopjunkie to switch a team that juz destroyed your own team. You juz don't!\n",
      "--------------------\n",
      "\n",
      "remove @user\n",
      "\n",
      "sample 1\n",
      "original: Rap that will Cut other raper's throat. Who said that? @Paedeezy #badd #wicked. #bright city lights\n",
      "cleaned : Rap that will Cut other raper's throat. Who said that? #badd #wicked. #bright city lights\n",
      "\n",
      "sample 2\n",
      "original: @lefluerr Lol yeah I read that but I stayed up and didn't nap tillI collect 3 people who I think they've got me ðŸ˜‚ðŸ˜‚!!\n",
      "cleaned : Lol yeah I read that but I stayed up and didn't nap tillI collect 3 people who I think they've got me ðŸ˜‚ðŸ˜‚!!\n",
      "\n",
      "sample 3\n",
      "original: I was having a #BadDay and i went on #YouTube and watched @domoandcrissy reaction to old pics. #Thanks #ladies for the #laughs\n",
      "cleaned : I was having a #BadDay and i went on #YouTube and watched reaction to old pics. #Thanks #ladies for the #laughs\n",
      "\n",
      "sample 4\n",
      "original: #ArchangelSummit @sethgodin  Anyone can be brave but you just have to last 5 mins longer than everyone else. #leadership \n",
      "cleaned : #ArchangelSummit Anyone can be brave but you just have to last 5 mins longer than everyone else. #leadership\n",
      "\n",
      "sample 5\n",
      "original: Long day, kind tweets, our melancholy, @DownFrontArtist defeats.\n",
      "cleaned : Long day, kind tweets, our melancholy, defeats.\n",
      "--------------------\n",
      "\n",
      "compress multiple spaces\n",
      "\n",
      "sample 1\n",
      "original: Swear to God don't get a smart meter from your power company, 8 months of daft bills, 6 visits from British Gas  #stressed\n",
      "cleaned : Swear to God don't get a smart meter from your power company, 8 months of daft bills, 6 visits from British Gas #stressed\n",
      "\n",
      "sample 2\n",
      "original: #ArchangelSummit @sethgodin  Anyone can be brave but you just have to last 5 mins longer than everyone else. #leadership \n",
      "cleaned : #ArchangelSummit Anyone can be brave but you just have to last 5 mins longer than everyone else. #leadership\n",
      "\n",
      "sample 3\n",
      "original: #food  #Ð´ÐµÐ½ÑŒÐ³Ð¸ #smile   microsoft_.net_framework_4.5.1_full_plus_by_gora\n",
      "cleaned : #food #Ð´ÐµÐ½ÑŒÐ³Ð¸ #smile microsoft_.net_framework_4.5.1_full_plus_by_gora\n",
      "\n",
      "sample 4\n",
      "original: BibleMotivate: Are you worrying/worried?\\n1Peter 5:7\\nThrow all your worry on him, because he cares for you. #faith #leadership  #mindfuâ€¦\n",
      "cleaned : BibleMotivate: Are you worrying/worried?\\n1Peter 5:7\\nThrow all your worry on him, because he cares for you. #faith #leadership #mindfuâ€¦\n",
      "\n",
      "sample 5\n",
      "original: @CI  I don't think Monalisa has respect for anyone but herself! I think she'll ruffle a few feathers. #TheJail\n",
      "cleaned : I don't think Monalisa has respect for anyone but herself! I think she'll ruffle a few feathers. #TheJail\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "def show_cleaning_examples(df, pattern, description):\n",
    "    print(f\"\\n{description}\")\n",
    "    mask = df[\"text\"].str.contains(pattern, regex = True, na = False)\n",
    "    samples = df[mask].head(5)\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        print(f\"\\nsample {idx}\")\n",
    "        print(\"original:\", row[\"text\"])\n",
    "        print(\"cleaned :\", row[\"clean_text\"])\n",
    "    \n",
    "    print(\"-\" * 20)\n",
    "\n",
    "show_cleaning_examples(train_df, r\"\\S+@\\S+\", \"remove email\")\n",
    "show_cleaning_examples(train_df, r\"@\\w+\", \"remove @user\")\n",
    "show_cleaning_examples(train_df, r\"\\s{2,}\", \"compress multiple spaces\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label mapping\n",
    "Creating a mapping between emotion strings and integer labels (label2id and id2label).\n",
    "- label2id: Maps emotion to integer, the result will be:\n",
    "    - **label2id: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}**\n",
    "- id2label: Maps integer back to emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:25.016703Z",
     "iopub.status.busy": "2025-12-01T12:24:25.016508Z",
     "iopub.status.idle": "2025-12-01T12:24:25.038131Z",
     "shell.execute_reply": "2025-12-01T12:24:25.037548Z",
     "shell.execute_reply.started": "2025-12-01T12:24:25.016686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}\n",
      "Full train_df_bert shape: (47890, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = train_df.copy()\n",
    "\n",
    "# label-to-id and id-to-label mapping\n",
    "unique_labels = sorted(df[\"emotion\"].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(\"label2id:\", label2id)\n",
    "df[\"label\"] = df[\"emotion\"].map(label2id)\n",
    "\n",
    "train_df_bert = df\n",
    "print(\"Full train_df_bert shape:\", train_df_bert.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer and dataset class\n",
    "\n",
    "- Load the **tokenizer** for the pre-trained model roberta-base. It can convert text to tokens that model can understand.\n",
    "    - In NLP course in NTHU, one assignment has experienced that **RoBERTa has better performance than BERT**, so I choose this.\n",
    "- Define **EmotionDataset class** to handle text and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:25.039115Z",
     "iopub.status.busy": "2025-12-01T12:24:25.038822Z",
     "iopub.status.idle": "2025-12-01T12:24:45.873181Z",
     "shell.execute_reply": "2025-12-01T12:24:45.872252Z",
     "shell.execute_reply.started": "2025-12-01T12:24:25.039073Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e448375903449aa4208e5014bb721f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8875ceaeafb24c81a4eb7557c021fec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af55b0ed7ff413880f51fe1bf336e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add42d2b8a26450f8e68be053df01fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95481e4bd064f9e809c6bcac50ff411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# define the model and load the tokenizer\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# PyTorch dataset class for emotion classification\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"clean_text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return{\n",
    "            \"text\": self.texts[idx],\n",
    "            \"label\": int(self.labels[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a collate_fn\n",
    "\n",
    "This function will be used by DataLoader to process samples(batch) from EmotionDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:24:45.874603Z",
     "iopub.status.busy": "2025-12-01T12:24:45.874109Z",
     "iopub.status.idle": "2025-12-01T12:24:45.879378Z",
     "shell.execute_reply": "2025-12-01T12:24:45.878627Z",
     "shell.execute_reply.started": "2025-12-01T12:24:45.874580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # extract text\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "\n",
    "    # tokenize the batch of texts\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        padding = \"longest\",\n",
    "        truncation = True,\n",
    "        max_length = 256,\n",
    "        return_tensors = \"pt\"\n",
    "    )\n",
    "\n",
    "    # for train/valid\n",
    "    if \"label\" in batch[0]:\n",
    "        enc[\"labels\"] = torch.tensor([item[\"label\"] for item in batch], dtype = torch.long)\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-fold cross-validation\n",
    "to ensure a robust training\n",
    "\n",
    "- split: divide data into train/validation\n",
    "- init: create a new roberta-base model for each fold\n",
    "- setup: using **AdamW optimizer** and **learning rate scheduler**\n",
    "- train: each epochs, calculate loss and update weights\n",
    "- validate: evaluate on the validation\n",
    "- save: only save model weights if it achieves new best f1 score in that fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T12:26:05.447946Z",
     "iopub.status.busy": "2025-12-01T12:26:05.447635Z",
     "iopub.status.idle": "2025-12-01T14:01:58.614057Z",
     "shell.execute_reply": "2025-12-01T14:01:58.613340Z",
     "shell.execute_reply.started": "2025-12-01T12:26:05.447925Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 12:26:10.900408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764591971.268672      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764591971.389990      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1551d509bb514c269f2b0df9dbe282a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "fold 1 epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:53<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.0190, val f1 = 0.5077\n",
      "Saved best model for fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 1 epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:56<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train loss = 0.8002, val f1 = 0.5144\n",
      "Saved best model for fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 1 epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:56<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: train loss = 0.6752, val f1 = 0.5323\n",
      "Saved best model for fold 1\n",
      "\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "fold 2 epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:57<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.0143, val f1 = 0.5161\n",
      "Saved best model for fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 2 epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:58<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train loss = 0.8017, val f1 = 0.5380\n",
      "Saved best model for fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 2 epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:57<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: train loss = 0.6764, val f1 = 0.5514\n",
      "Saved best model for fold 2\n",
      "\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "fold 3 epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:56<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.0214, val f1 = 0.5105\n",
      "Saved best model for fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 3 epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:56<00:00,  6.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train loss = 0.7918, val f1 = 0.5169\n",
      "Saved best model for fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 3 epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:56<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: train loss = 0.6670, val f1 = 0.5246\n",
      "Saved best model for fold 3\n",
      "\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "fold 4 epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:57<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.0162, val f1 = 0.4855\n",
      "Saved best model for fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 4 epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:57<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train loss = 0.7990, val f1 = 0.5361\n",
      "Saved best model for fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 4 epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:58<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: train loss = 0.6712, val f1 = 0.5375\n",
      "Saved best model for fold 4\n",
      "\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "fold 5 epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:55<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: train loss = 1.0142, val f1 = 0.5068\n",
      "Saved best model for fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 5 epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:56<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: train loss = 0.7949, val f1 = 0.5090\n",
      "Saved best model for fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 5 epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2395/2395 [05:55<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: train loss = 0.6726, val f1 = 0.5189\n",
      "Saved best model for fold 5\n",
      "average f1 score: 0.5329422213969982\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# k-fold parameter\n",
    "N_FOLDS = 5\n",
    "skf = StratifiedKFold(n_splits = N_FOLDS, shuffle = True, random_state = 42)\n",
    "num_labels = len(label2id)\n",
    "num_epochs = 3\n",
    "\n",
    "# store the best f1 of each fold\n",
    "fold_best_f1s = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df_bert, train_df_bert[\"label\"])):\n",
    "    print(f\"\\nFold {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    # slice the dataframe\n",
    "    fold_train_df = train_df_bert.iloc[train_idx].reset_index(drop = True)\n",
    "    fold_val_df = train_df_bert.iloc[val_idx].reset_index(drop = True)\n",
    "    \n",
    "    # dataset and DataLoader\n",
    "    train_dataset = EmotionDataset(fold_train_df)\n",
    "    val_dataset = EmotionDataset(fold_val_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True, collate_fn = collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False, collate_fn = collate_fn)\n",
    "    \n",
    "    # initialize model for each fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        num_labels = num_labels,\n",
    "        id2label = id2label,\n",
    "        label2id = label2id\n",
    "    ).to(device)\n",
    "    \n",
    "    # optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-5, weight_decay = 0.01)\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps = warmup_steps, num_training_steps = total_steps\n",
    "    )\n",
    "    \n",
    "    # training loop\n",
    "    best_val_f1 = -1.0\n",
    "    best_checkpoint_path = f\"model_fold_{fold}.pt\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc = f\"fold {fold+1} epoch {epoch+1}\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward() # backward pass\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels = labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # get predictions\n",
    "                preds = torch.argmax(outputs.logits, dim = -1)\n",
    "                all_preds.append(preds.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        # macro F1 Score\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average = \"macro\")\n",
    "        \n",
    "        print(f\"epoch {epoch+1}: train loss = {avg_train_loss:.4f}, val f1 = {val_f1:.4f}\")\n",
    "\n",
    "        # save best model if need\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), best_checkpoint_path)\n",
    "            print(f\"Saved best model for fold {fold+1}\")\n",
    "            \n",
    "    fold_best_f1s.append(best_val_f1)\n",
    "    \n",
    "    del model, optimizer, scheduler, train_loader, val_loader\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"average f1 score:\", np.mean(fold_best_f1s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test dataset and DataLoader\n",
    "- Define a TestDataset class for test set, which does not require labels.\n",
    "- Initialize the test dataset and its DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T14:01:58.615851Z",
     "iopub.status.busy": "2025-12-01T14:01:58.615275Z",
     "iopub.status.idle": "2025-12-01T14:01:58.623150Z",
     "shell.execute_reply": "2025-12-01T14:01:58.622399Z",
     "shell.execute_reply.started": "2025-12-01T14:01:58.615831Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset size: 16281\n"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"clean_text\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"text\": self.texts[idx]}\n",
    "\n",
    "# create test dataset and DataLoader\n",
    "test_dataset = TestDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 16, shuffle = False, collate_fn = collate_fn)\n",
    "\n",
    "print(\"test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using ensemble approach\n",
    "\n",
    "- predict probabilities for test set with each model\n",
    "- then average the probabilities of N models\n",
    "- select the one with the highest average to get more robust probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T14:01:58.624744Z",
     "iopub.status.busy": "2025-12-01T14:01:58.624515Z",
     "iopub.status.idle": "2025-12-01T14:05:39.801098Z",
     "shell.execute_reply": "2025-12-01T14:05:39.800337Z",
     "shell.execute_reply.started": "2025-12-01T14:01:58.624729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict with fold 0 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "fold 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1018/1018 [00:43<00:00, 23.42it/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict with fold 1 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1018/1018 [00:43<00:00, 23.39it/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict with fold 2 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1018/1018 [00:43<00:00, 23.41it/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict with fold 3 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1018/1018 [00:43<00:00, 23.40it/s]\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict with fold 4 model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fold 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1018/1018 [00:43<00:00, 23.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "all_fold_probs = []\n",
    "\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f\"predict with fold {fold+1} model:\")\n",
    "    checkpoint_path = f\"model_fold_{fold}.pt\"\n",
    "    \n",
    "    # initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"roberta-base\",\n",
    "        num_labels = len(label2id),\n",
    "        id2label = id2label,\n",
    "        label2id = label2id\n",
    "    ).to(device)\n",
    "    \n",
    "    # the best weights for this fold\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "    model.eval()\n",
    "    \n",
    "    fold_probs = []\n",
    "    \n",
    "    # prediction\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc = f\"fold {fold+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            # use softmax to get probability distribution\n",
    "            probs = F.softmax(outputs.logits, dim = -1)\n",
    "            fold_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "    # concatenate all batch predictions\n",
    "    all_fold_probs.append(np.concatenate(fold_probs))\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# ensemble, take the average of probability and select the maximum\n",
    "avg_probs = np.mean(all_fold_probs, axis = 0)\n",
    "final_preds = np.argmax(avg_probs, axis = -1)\n",
    "\n",
    "# id back to labels\n",
    "pred_labels = [id2label[p] for p in final_preds]\n",
    "id_to_pred = {id_: label for id_, label in zip(test_df[\"id\"], pred_labels)}\n",
    "\n",
    "# submission file\n",
    "sample_sub = pd.read_csv(f\"{path}/samplesubmission.csv\")\n",
    "submission = sample_sub[[\"id\"]].copy()\n",
    "submission[\"emotion\"] = submission[\"id\"].map(id_to_pred)\n",
    "submission.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13999840,
     "sourceId": 117189,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
